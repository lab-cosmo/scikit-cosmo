{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-Importance-of-Data-Scaling-in-PCovR-/-KernelPCovR\" data-toc-modified-id=\"The-Importance-of-Data-Scaling-in-PCovR-/-KernelPCovR-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The Importance of Data Scaling in PCovR / KernelPCovR</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Importance of Data Scaling in PCovR / KernelPCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skcosmo.decomposition import PCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCovR, and KernelPCovR, we are combining multiple aspects of the dataset, primarily the features and targets. \n",
    "As such, the results largely depend on the relative contributions of each aspect to the mixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the boston housing prices from sklearn. In their raw form, the magnitudes of the features and targets are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\n",
    "    \"Norm of the features: %0.2f \\nNorm of the targets: %0.2f\"\n",
    "    % (np.linalg.norm(X), np.linalg.norm(y))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the boston dataset, we can use the `StandardScaler` class from sklearn, as the features and targets are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_scaled = x_scaler.fit_transform(X)\n",
    "y_scaled = y_scaler.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results at `mixing=0.5`, we see an especially large difference in the latent-space projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcovr_unscaled = PCovR(mixing=0.5).fit(X, y)\n",
    "T_unscaled = pcovr_unscaled.transform(X)\n",
    "Yp_unscaled = pcovr_unscaled.predict(X)\n",
    "\n",
    "pcovr_scaled = PCovR(mixing=0.5).fit(X_scaled, y_scaled)\n",
    "T_scaled = pcovr_scaled.transform(X_scaled)\n",
    "Yp_scaled = y_scaler.inverse_transform(pcovr_scaled.predict(X_scaled))\n",
    "\n",
    "\n",
    "fig, ((ax1_T, ax2_T), (ax1_Y, ax2_Y)) = plt.subplots(2, 2, figsize=(8, 10))\n",
    "\n",
    "ax1_T.scatter(T_unscaled[:, 0], T_unscaled[:, 1], c=y, cmap=\"plasma\", ec=\"k\")\n",
    "ax1_T.set_xlabel(\"PCov1\")\n",
    "ax1_T.set_ylabel(\"PCov2\")\n",
    "ax1_T.set_title(\"Latent Projection\\nWithout Scaling\")\n",
    "\n",
    "ax2_T.scatter(T_scaled[:, 0], T_scaled[:, 1], c=y, cmap=\"plasma\", ec=\"k\")\n",
    "ax2_T.set_xlabel(\"PCov1\")\n",
    "ax2_T.set_ylabel(\"PCov2\")\n",
    "ax2_T.set_title(\"Latent Projection\\nWith Scaling\")\n",
    "\n",
    "ax1_Y.scatter(Yp_unscaled, y, c=np.abs(y - Yp_unscaled), cmap=\"bone_r\", ec=\"k\")\n",
    "ax1_Y.plot(ax1_Y.get_xlim(), ax1_Y.get_xlim(), \"r--\")\n",
    "ax1_Y.set_xlabel(\"True Y, unscaled\")\n",
    "ax1_Y.set_ylabel(\"Predicted Y, unscaled\")\n",
    "ax1_Y.set_title(\"Regression\\nWithout Scaling\")\n",
    "\n",
    "ax2_Y.scatter(Yp_scaled, y, c=np.abs(y.flatten() - Yp_scaled.flatten()), cmap=\"bone_r\", ec=\"k\")\n",
    "ax2_Y.plot(ax2_Y.get_xlim(), ax2_Y.get_xlim(), \"r--\")\n",
    "ax2_Y.set_xlabel(\"True Y, unscaled\")\n",
    "ax2_Y.set_ylabel(\"Predicted Y, unscaled\")\n",
    "ax2_Y.set_title(\"Regression\\nWith Scaling\")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we see that the total loss (loss in recreating the original dataset and regression loss) does not vary with `mixing` as expected when the datasets are unscaled. Typically, the regression loss should _gradually_ increase with `mixing` (and vice-versa for the loss in reconstructing the original features). When the inputs are not scaled, however, there will often be a sharp drop or jump in the losses as `mixing` goes to 0 or 1, depending on which input is dominating the results of the model. Here, because the features dominate the model, this jump occurs as `mixing` goes to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixings = np.linspace(0, 1, 21)\n",
    "losses_unscaled = np.zeros((2, len(mixings)))\n",
    "losses_scaled = np.zeros((2, len(mixings)))\n",
    "\n",
    "for mi, mixing in enumerate(mixings):\n",
    "    pcovr_unscaled = PCovR(mixing=mixing, n_components=1).fit(X, y)\n",
    "    t_unscaled = pcovr_unscaled.transform(X)\n",
    "    yp_unscaled = pcovr_unscaled.predict(T=t_unscaled)\n",
    "    xr_unscaled = pcovr_unscaled.inverse_transform(t_unscaled)\n",
    "    losses_unscaled[:, mi] = (\n",
    "        np.linalg.norm(xr_unscaled - X) ** 2.0 / np.linalg.norm(X) ** 2,\n",
    "        np.linalg.norm(yp_unscaled - y) ** 2.0 / np.linalg.norm(y) ** 2,\n",
    "    )\n",
    "\n",
    "    pcovr_scaled = PCovR(mixing=mixing, n_components=1).fit(X_scaled, y_scaled)\n",
    "    t_scaled = pcovr_scaled.transform(X_scaled)\n",
    "    yp_scaled = y_scaler.inverse_transform(pcovr_scaled.predict(T=t_scaled))\n",
    "    xr_scaled = x_scaler.inverse_transform(pcovr_scaled.inverse_transform(t_scaled))\n",
    "    losses_scaled[:, mi] = (\n",
    "        np.linalg.norm(xr_scaled - X) ** 2.0 / np.linalg.norm(X) ** 2,\n",
    "        np.linalg.norm(yp_scaled - y) ** 2.0 / np.linalg.norm(y) ** 2,\n",
    "    )\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True, sharex=True)\n",
    "ax1.semilogy(mixings, losses_unscaled[0], marker=\"o\", label=r\"$\\ell_{X}$\")\n",
    "ax1.semilogy(mixings, losses_unscaled[1], marker=\"o\", label=r\"$\\ell_{Y}$\")\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_title('With Inputs Unscaled')\n",
    "ax1.set_xlabel(r'Mixing parameter $\\alpha$')\n",
    "ax1.set_ylabel(r'Loss $\\ell$')\n",
    "\n",
    "ax2.semilogy(mixings, losses_scaled[0], marker=\"o\", label=r\"$\\ell_{X}$\")\n",
    "ax2.semilogy(mixings, losses_scaled[1], marker=\"o\", label=r\"$\\ell_{Y}$\")\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.set_title('With Inputs Scaled')\n",
    "ax2.set_xlabel(r'Mixing parameter $\\alpha$')\n",
    "ax2.set_ylabel(r'Loss $\\ell$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: when the relative magnitude of the features or targets is important, such as in load_csd_1000r, one should use the `StandardFlexibleScaler` provided by `scikit-cosmo`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
